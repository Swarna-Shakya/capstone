{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R9bcJt7nusw-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6_pXPpouwlf"
      },
      "outputs": [],
      "source": [
        "questions_df = pd.read_csv(\"/content/CHATBOT CAPSTONE DATASET.csv\", encoding='unicode_escape')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-uo7kTnuzEg"
      },
      "outputs": [],
      "source": [
        "answers_df = pd.read_csv(\"/content/CHATBOT CAPSTONE DATASET.csv\", encoding='unicode_escape')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe6E9DoVu2DY",
        "outputId": "fddd1580-805d-4c29-b651-9814cdec14ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNQUYcVGu4Mt"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16jAABtqu68l"
      },
      "outputs": [],
      "source": [
        "def preprocess_with_stopwords(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3vija-Gu-iN"
      },
      "outputs": [],
      "source": [
        "questions_df = pd.read_csv(\"/content/CHATBOT CAPSTONE DATASET.csv\", encoding='unicode_escape')  # Specify the encoding\n",
        "questions_list = questions_df['Questions'].tolist()  # Assuming 'Question' is the column name containing questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaxJH8f0vCed",
        "outputId": "0b844679-38a4-4f03-9685-7aa67764a747"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the missing 'punkt_tab' resource\n",
        "\n",
        "def preprocess(text):\n",
        "    # Check if the input is a string, if not convert it\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove non-alphanumeric characters\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list]) # This line caused the error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Cki0CnvFDN"
      },
      "outputs": [],
      "source": [
        "def get_response(user_query):\n",
        "    processed_text = preprocess_with_stopwords(user_query)\n",
        "    vectorized_text = vectorizer.transform([processed_text])\n",
        "    similarities = cosine_similarity(vectorized_text, X)\n",
        "    max_similarity = np.max(similarities)\n",
        "\n",
        "    if max_similarity > 0.6:\n",
        "        # Find high similarity questions\n",
        "        high_similarity_indices = [i for i, s in enumerate(similarities[0]) if s > 0.6]\n",
        "        high_similarity_questions = [questions_list[i] for i in high_similarity_indices]\n",
        "\n",
        "        # Retrieve corresponding answers\n",
        "        target_answers = [answers_list[i] for i in high_similarity_indices]\n",
        "\n",
        "        # Further refine based on corrected text\n",
        "        Z = vectorizer.transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
        "        vectorized_text_with_stopwords = vectorizer.transform([processed_text])\n",
        "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
        "        closest_index = final_similarities.argmax()\n",
        "\n",
        "        return target_answers[closest_index]\n",
        "    else:\n",
        "        return \"I don't have information on that. Please ask something else.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTOI6t8_vKVl",
        "outputId": "d17cefae-5749-4ec5-b2c7-0ac6d1f229ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! I'm a assistant chatbot for IIMS Hotel. How can I help you?\n",
            "Chatbot: The price of Family room is Rs.3500 per night?\n",
            "Chatbot: We offer 3 different rooms which are Super comfort, Duplex and family.Which room prices would you want to know?\n",
            "Chatbot: I don't have information on that. Please ask something else.\n",
            "Chatbot: ItÂ’s a budget-friendly room with twin beds and basic amenities.\n",
            "Chatbot: Hello! Do you have any inquries?\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def preprocess(text):\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "def preprocess_with_stopwords(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]\n",
        "    return ' '.join(stemmed_tokens)\n",
        "\n",
        "# Assuming your CSV has a column named 'Answers'\n",
        "questions_df = pd.read_csv(\"/content/CHATBOT CAPSTONE DATASET.csv\", encoding='unicode_escape')\n",
        "questions_list = questions_df['Questions'].tolist()\n",
        "answers_list = questions_df['Answers'].tolist()  # Extract answers\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
        "X = vectorizer.fit_transform([preprocess(q) for q in questions_list])\n",
        "\n",
        "def get_response(user_query):\n",
        "    processed_text = preprocess_with_stopwords(user_query)\n",
        "    vectorized_text = vectorizer.transform([processed_text])\n",
        "    similarities = cosine_similarity(vectorized_text, X)\n",
        "    max_similarity = np.max(similarities)\n",
        "\n",
        "    if max_similarity > 0.6:\n",
        "        high_similarity_indices = [i for i, s in enumerate(similarities[0]) if s > 0.6]\n",
        "\n",
        "        # Ensure index is within bounds of answers_list\n",
        "        high_similarity_indices = [i for i in high_similarity_indices if i < len(answers_list)]\n",
        "\n",
        "        high_similarity_questions = [questions_list[i] for i in high_similarity_indices]\n",
        "        target_answers = [answers_list[i] for i in high_similarity_indices]\n",
        "\n",
        "        Z = vectorizer.transform([preprocess_with_stopwords(q) for q in high_similarity_questions])\n",
        "        vectorized_text_with_stopwords = vectorizer.transform([processed_text])\n",
        "        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)\n",
        "\n",
        "        # Check if final_similarities is empty before calling argmax\n",
        "        if final_similarities.size > 0:\n",
        "            closest_index = final_similarities.argmax()\n",
        "            return target_answers[closest_index]\n",
        "        else:\n",
        "            return \"I don't have information on that. Please ask something else.\"\n",
        "    else:\n",
        "        return \"I don't have information on that. Please ask something else.\"\n",
        "\n",
        "print(\"Hello! I'm a assistant chatbot for IIMS Hotel. How can I help you?\")\n",
        "while True:\n",
        "    user_query = input(\"You: \")\n",
        "    if user_query.lower() in ['exit', 'quit', 'bye']:\n",
        "        print(\"Chatbot: Goodbye!\")\n",
        "        break\n",
        "    response = get_response(user_query)\n",
        "    print(f\"Chatbot: {response}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}